Генератор диалогов AI (Dialogue Generator AI)
Веб-приложение для генерации диалогов между двумя персонажами с использованием модели крупного языка (LLM) Grok-3-reasoning-gemma3. Приложение построено на Flask и SocketIO, позволяя пользователям инициировать, контролировать и наблюдать за разговором в реальном времени. Диалоги генерируются на основе заданной темы и описаний ролей персонажей, с возможностью мониторинга загрузки GPU и сохранения результатов.

Особенности
Реал-тайм диалоги: Использует WebSocket (SocketIO) для мгновенной генерации и отображения реплик.
Кастомизируемые роли: Пользователи могут задавать имена и детальные описания двух персонажей.
Генерация на LLM: Интегрирована модель Grok-3-reasoning-gemma3 (GGUF формат)
через библиотеку llama_cpp для высококачественной генерации текста.
Мониторинг GPU: Встроенный индикатор загрузки GPU с автоматическими обновлениями статуса.
Контроль процессов: Возможность остановки диалога в любое время.
Сохранение диалогов: Автоматическое сохранение полных диалогов в файлы с метаданными (тема, роли, временные метки).
Безопасность: Поддержка HTTPS (с SSL сертификатами); fallback на HTTP при отсутствии сертификатов.
Блокировка диалогов: Одиночный диалог за раз для предотвращения конфликтов ресурсов.
Адаптивный UI: Мобильно-адаптивный интерфейс с темной темой при необходимости.

Технологии:
Backend: Python, Flask, Flask-SocketIO, llama_cpp (для LLM), GPUtil (мониторинг GPU), threading, ssl.
Frontend: HTML, CSS, JavaScript, Socket.IO (клиентская часть).
Модель LLM: Grok-3-reasoning-gemma3-12B-distilled-HF.Q8_0 (локальная модель в формате GGUF, размещение по умолчанию: G:\LLM_models2\).
Тестировано на: RTX 3090ti, время генерации ответа: 8-17 сек.

Установка и Запуск:
Требования
Python 3.8+
GPU с поддержкой CUDA (рекомендуется для оптимальной производительности модели).
Библиотеки: Установите зависимости из requirements.txt (если файл отсутствует, создайте его с содержимым ниже).
Модель GGUF: Скачайте и разместите файл Grok-3-reasoning-gemma3-12B-distilled-HF.Q8_0.gguf по пути G:\LLM_models2\ (или измените путь в app.py).
Шаг 1: Клонирование репозитория
git clone https://github.com/your-username/dialogue-generator.git
cd dialogue-generator
Шаг 2: Установка зависимостей
Создайте виртуальное окружение (рекомендуется):

python -m venv venv
source venv/bin/activate  # На Windows: venv\Scripts\activate
Установите пакеты:

pip install flask flask-socketio llama-cpp-python GPUtil
Шаг 3: Настройка SSL (опционально для HTTPS)
Для запуска в режиме HTTPS создайте SSL-сертификаты:

openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365
Сертификаты должны находиться в корневой директории проекта.

Шаг 4: Запуск приложения
Запустите сервер:

python app.py
По умолчанию: HTTP на порту 5000.
С сертификатами: HTTPS на порту 443.
Откройте браузер и перейдите на http://localhost:5000 (или https://localhost при HTTPS).

Использование
Заполните параметры: Введите тему диалога, имена и описания ролей, количество шагов (реплик).
Запустите диалог: Нажмите "Запустить диалог". Реплики будут отображаться в реальном времени с прогресс-баром.
Мониторьте GPU: Индикатор покажет статус GPU (свободен/занят).
Остановите при необходимости: Кнопка "Стоп" прерывает генерацию.
Просмотр результатов: Диалоги сохраняются автоматически в папке logs/ в формате TXT с метаданными.

Пример ввода:
Тема: Спор о форме Земли
Роль 1 (Ученый): Ты известный ученый физик, аргументируй логически.
Роль 2 (Студент): Ты веселый студент, шутй и приводи примеры.
Шаги: 7

Пример вывода:

1. Ученый: Привет, давай поспорим?
2. Студент: Студент: Ха-ха, конечно поспорим! У меня всегда найдется аргумент,
   чтобы доказать, что Земля — шар, а не плоский диск, как некоторые любят считать!
3. Ученый: Ну что, юноша, ты готов принять факт, что Земля — это геоид, а не плоский диск,
   как будто мы живем на старой монете? У меня есть доказательства,
   которые сотрясут твою веру в эту глупость!
4. Студент: [генерация ответа]
...

Структура проекта:
app.py: Основной серверный файл с логикой Flask и SocketIO.
index.html: Клиентский интерфейс (шаблон Flask, но здесь самостоятельный для демо).
logs/: Папка для сохраненных диалогов (создается автоматически).
Лицензия
MIT License. Смотрите файл LICENSE для деталей.

Конвенции кода:
Код на Python: PEP 8.
Логирование: Используется logging для отладки.
Безопасность: Многопоточность с блокировками; уникальные SID для пользователей.
Known Issues
Генерация зависит от мощности LSTM: Медленно на слабом GPU (8-17 сек на RTX 3090ti).
Модель жестко закодирована: Поддержка других моделей требует изменений в load_model().
Одиночный диалог: Блокировка предотвращает параллельные запуски.

Будущие улучшения:
Поддержка нескольких моделей LLM.
Добавление инструментов (Калькулятор, поиск в интернете и т.д)
Расширение на >2 ролей.
Интеграция с облачными API (опционально).
Тестирование на большем количестве GPU.
Интеграция с telegram.
Интеграция с n8n
Для вопросов или вклада: создайте Issue или Pull Request на GitHub.
Готов запустить код для демонстрации по запросу в telegram @sysneld
